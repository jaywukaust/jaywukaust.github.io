<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=google-site-verification content="7MghMiEezWCdLZfbrgYR845yxCJj-g02bSAnoythP1A"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.5.0"><meta name=author content="Dai-Jie Wu"><meta name=description content="Kinship face synthesis is a challenging and ill-posed problem due to a scarce amount of available kinship data and low quality. To address these issues, we propose the Style Latent Diffusion Transformer (StyleDiT), a novel framework to integrate the strengths of StyleGAN with the diffusion model to generate high-fidelity and diverse kinship faces, where StyleGAN is responsible for final face generation and our conditional diffusion model is used to sample a StyleGAN latent adhering to the characteristics of condition images. Another key innovation of our work is the Relational Trait Guidance (RTG) mechanism, a Classifier-Free Guidance approach for our diffusion transformer. RTG enables independent control of influencing conditions, such as each parent's facial image, allowing for a fine-grained control between diversity and fidelity in the synthesized faces. This flexibility is crucial for applications requiring specific attribute emphasis. Furthermore, this research extends its application to an unexplored domain: predicting a partner's facial features by synthesizing the combined imagery of a child and one parent. Finally, through extensive quantitative, qualitative, and subject evaluations, StyleDiT demonstrates superior performance in synthesizing diverse and high-fidelity kinship faces compared to existing methods."><link rel=alternate hreflang=en-us href=https://jaywukaust.github.io/publication/cvpr2024styledit/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.min.7ff2359a5a6c81fb86a941f3d6a32bce.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-100690611-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","UA-100690611-1",{}),document.addEventListener("click",onClickCallback,!1)</script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/img/icon.ico><link rel=apple-touch-icon type=image/png href=/img/icon.ico><link rel=canonical href=https://jaywukaust.github.io/publication/cvpr2024styledit/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Dai-Jie Wu"><meta property="og:url" content="https://jaywukaust.github.io/publication/cvpr2024styledit/"><meta property="og:title" content="StyleDiT: A Unified Framework for Diverse Kinship Faces Synthesis with Style Latent Diffusion Transformer | Dai-Jie Wu"><meta property="og:description" content="Kinship face synthesis is a challenging and ill-posed problem due to a scarce amount of available kinship data and low quality. To address these issues, we propose the Style Latent Diffusion Transformer (StyleDiT), a novel framework to integrate the strengths of StyleGAN with the diffusion model to generate high-fidelity and diverse kinship faces, where StyleGAN is responsible for final face generation and our conditional diffusion model is used to sample a StyleGAN latent adhering to the characteristics of condition images. Another key innovation of our work is the Relational Trait Guidance (RTG) mechanism, a Classifier-Free Guidance approach for our diffusion transformer. RTG enables independent control of influencing conditions, such as each parent's facial image, allowing for a fine-grained control between diversity and fidelity in the synthesized faces. This flexibility is crucial for applications requiring specific attribute emphasis. Furthermore, this research extends its application to an unexplored domain: predicting a partner's facial features by synthesizing the combined imagery of a child and one parent. Finally, through extensive quantitative, qualitative, and subject evaluations, StyleDiT demonstrates superior performance in synthesizing diverse and high-fidelity kinship faces compared to existing methods."><meta property="og:image" content="https://jaywukaust.github.io/publication/cvpr2024styledit/featured.jpeg"><meta property="twitter:image" content="https://jaywukaust.github.io/publication/cvpr2024styledit/featured.jpeg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-12-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-24T14:36:06+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jaywukaust.github.io/publication/cvpr2024styledit/"},"headline":"StyleDiT: A Unified Framework for Diverse Kinship Faces Synthesis with Style Latent Diffusion Transformer","image":["https://jaywukaust.github.io/publication/cvpr2024styledit/featured.jpeg"],"datePublished":"2023-12-01T00:00:00Z","dateModified":"2023-12-24T14:36:06+08:00","author":{"@type":"Person","name":"Pin-Yen Chiu*"},"publisher":{"@type":"Organization","name":"Dai-Jie Wu","logo":{"@type":"ImageObject","url":"https://jaywukaust.github.io/img/icon.ico"}},"description":"Kinship face synthesis is a challenging and ill-posed problem due to a scarce amount of available kinship data and low quality. To address these issues, we propose the Style Latent Diffusion Transformer (StyleDiT), a novel framework to integrate the strengths of StyleGAN with the diffusion model to generate high-fidelity and diverse kinship faces, where StyleGAN is responsible for final face generation and our conditional diffusion model is used to sample a StyleGAN latent adhering to the characteristics of condition images. Another key innovation of our work is the Relational Trait Guidance (RTG) mechanism, a Classifier-Free Guidance approach for our diffusion transformer. RTG enables independent control of influencing conditions, such as each parent's facial image, allowing for a fine-grained control between diversity and fidelity in the synthesized faces. This flexibility is crucial for applications requiring specific attribute emphasis. Furthermore, this research extends its application to an unexplored domain: predicting a partner's facial features by synthesizing the combined imagery of a child and one parent. Finally, through extensive quantitative, qualitative, and subject evaluations, StyleDiT demonstrates superior performance in synthesizing diverse and high-fidelity kinship faces compared to existing methods."}</script><title>StyleDiT: A Unified Framework for Diverse Kinship Faces Synthesis with Style Latent Diffusion Transformer | Dai-Jie Wu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Dai-Jie Wu</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#accomplishments><span>Accomplishments</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class=pub><div class="article-container pt-3"><h1>StyleDiT: A Unified Framework for Diverse Kinship Faces Synthesis with Style Latent Diffusion Transformer</h1><div class=article-metadata><div><span>Pin-Yen Chiu*</span>, <span><strong><strong>Dai-Jie Wu</strong>*</strong></span>, <span>Chia-Hsuan Hsu</span>, <span>Hsiang-Chen Chiu</span>, <span>Chih-Yu Wang</span>, <span>Jun-Cheng Chen</span></div><span class=article-date>December 2023</span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://jaywukaust.github.io/publication/cvpr2024styledit/&text=StyleDiT:%20A%20Unified%20Framework%20for%20Diverse%20Kinship%20Faces%20Synthesis%20with%20Style%20Latent%20Diffusion%20Transformer" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://jaywukaust.github.io/publication/cvpr2024styledit/&t=StyleDiT:%20A%20Unified%20Framework%20for%20Diverse%20Kinship%20Faces%20Synthesis%20with%20Style%20Latent%20Diffusion%20Transformer" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=StyleDiT:%20A%20Unified%20Framework%20for%20Diverse%20Kinship%20Faces%20Synthesis%20with%20Style%20Latent%20Diffusion%20Transformer&body=https://jaywukaust.github.io/publication/cvpr2024styledit/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://jaywukaust.github.io/publication/cvpr2024styledit/&title=StyleDiT:%20A%20Unified%20Framework%20for%20Diverse%20Kinship%20Faces%20Synthesis%20with%20Style%20Latent%20Diffusion%20Transformer" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=StyleDiT:%20A%20Unified%20Framework%20for%20Diverse%20Kinship%20Faces%20Synthesis%20with%20Style%20Latent%20Diffusion%20Transformer%20https://jaywukaust.github.io/publication/cvpr2024styledit/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://jaywukaust.github.io/publication/cvpr2024styledit/&title=StyleDiT:%20A%20Unified%20Framework%20for%20Diverse%20Kinship%20Faces%20Synthesis%20with%20Style%20Latent%20Diffusion%20Transformer" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></div><div class="article-container pt-3 featured-image-wrapper mt-4 mb-4"><div style=position:relative><img src=/publication/cvpr2024styledit/featured_hub97e0f29db2b683bd20375e07c767829_721521_1200x0_resize_q90_lanczos.jpeg alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Kinship face synthesis is a challenging and ill-posed problem due to a scarce amount of available kinship data and low quality. To address these issues, we propose the Style Latent Diffusion Transformer (StyleDiT), a novel framework to integrate the strengths of StyleGAN with the diffusion model to generate high-fidelity and diverse kinship faces, where StyleGAN is responsible for final face generation and our conditional diffusion model is used to sample a StyleGAN latent adhering to the characteristics of condition images. Another key innovation of our work is the Relational Trait Guidance (RTG) mechanism, a Classifier-Free Guidance approach for our diffusion transformer. RTG enables independent control of influencing conditions, such as each parent&rsquo;s facial image, allowing for a fine-grained control between diversity and fidelity in the synthesized faces. This flexibility is crucial for applications requiring specific attribute emphasis. Furthermore, this research extends its application to an unexplored domain: predicting a partner&rsquo;s facial features by synthesizing the combined imagery of a child and one parent. Finally, through extensive quantitative, qualitative, and subject evaluations, StyleDiT demonstrates superior performance in synthesizing diverse and high-fidelity kinship faces compared to existing methods.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024), Under Review</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tags/kinship-face-generation/>Kinship Face Generation</a>
<a class="badge badge-light" href=/tags/transformer/>Transformer</a>
<a class="badge badge-light" href=/tags/diffusion/>Diffusion</a>
<a class="badge badge-light" href=/tags/stylegan/>StyleGAN</a></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",talk:"Talks"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/academic.min.5c1ab144ecb9ec087e4ea0940632fbec.js></script><div class=container><footer class=site-footer><p class=powered-by>© 2023 Dai-Jie Wu. Thanks for the template from wowchemy-hugo-themes.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>