[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1703399766,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jaywukaust.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Dai-Jie Wu","type":"authors"},{"authors":["__**Dai-Jie Wu**__","Guocheng Qian","Tingting Liao","Qian Wang","Silvio Giancola","Peter Wonka","Sergey Tulyakov","Kfir Aberman","Hao Li","Bernard Ghanem"],"categories":null,"content":"","date":1703376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703399766,"objectID":"dc8fb0691d07ca5fe45f17a00ca73af2","permalink":"https://jaywukaust.github.io/publication/ongoing2023vta360/","publishdate":"2023-12-24T00:00:00Z","relpermalink":"/publication/ongoing2023vta360/","section":"publication","summary":"Existing video-to-avatar systems predominantly focus on reconstructing head avatars within the field of view of the input videos, which impedes the generation of complete, all-rounded 3D avatars that can render from any novel views. We introduce an innovative framework, video-to-avatar360, to reconstruct a full 360-degree implicit neural head avatar from a single monocular frontal video. We first propose to use the signed distance function extracted from the prior mesh as a condition to the canonical Neural Radiance Field, to provide a coarse clue for the geometry of the full head avatar. We then incorporate a personalized diffusion guidance, DreamBooth-SDS, to optimize the NeRF in the novel views while being identity-preserving. Video-to-Avatar360 enables the generation of photorealistic 360-degree avatars, achieving a substantial performance leap over the state-of-the-art, for both reference-view reconstruction and novel view synthesis.","tags":["AIGC","3D","Avatar"],"title":"VTA360: 360° Head Avatar Generation from Monocular Frontal Videos","type":"publication"},{"authors":["Pin-Yen Chiu*","__**Dai-Jie Wu***__","Chia-Hsuan Hsu","Hsiang-Chen Chiu","Chih-Yu Wang","Jun-Cheng Chen"],"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703399766,"objectID":"d41339fdfb11782da64f079851b64de0","permalink":"https://jaywukaust.github.io/publication/cvpr2024styledit/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/cvpr2024styledit/","section":"publication","summary":"Kinship face synthesis is a challenging and ill-posed problem due to a scarce amount of available kinship data and low quality. To address these issues, we propose the Style Latent Diffusion Transformer (StyleDiT), a novel framework to integrate the strengths of StyleGAN with the diffusion model to generate high-fidelity and diverse kinship faces, where StyleGAN is responsible for final face generation and our conditional diffusion model is used to sample a StyleGAN latent adhering to the characteristics of condition images. Another key innovation of our work is the Relational Trait Guidance (RTG) mechanism, a Classifier-Free Guidance approach for our diffusion transformer. RTG enables independent control of influencing conditions, such as each parent's facial image, allowing for a fine-grained control between diversity and fidelity in the synthesized faces. This flexibility is crucial for applications requiring specific attribute emphasis. Furthermore, this research extends its application to an unexplored domain: predicting a partner's facial features by synthesizing the combined imagery of a child and one parent. Finally, through extensive quantitative, qualitative, and subject evaluations, StyleDiT demonstrates superior performance in synthesizing diverse and high-fidelity kinship faces compared to existing methods.","tags":["Kinship Face Generation","Transformer","Diffusion","StyleGAN"],"title":"StyleDiT: A Unified Framework for Diverse Kinship Faces Synthesis with Style Latent Diffusion Transformer","type":"publication"},{"authors":["Jingyu Zhang","Huitong Yang","__**Dai-Jie Wu**__","Jacky Keung","Xuesong Li","Xinge Zhu","Yuexin Ma"],"categories":null,"content":"","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703399766,"objectID":"156771aec57255d40d283cdece089147","permalink":"https://jaywukaust.github.io/publication/prcv2023crossmodal/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/prcv2023crossmodal/","section":"publication","summary":"Current state-of-the-art point cloud-based perception methods usually rely on large-scale labeled data, which requires expensive manual annotations. A natural option is to explore the unsupervised methodology for 3D perception tasks. However, such methods often face substantial performance-drop difficulties. Fortunately, we found that there exist amounts of image-based datasets and an alternative can be proposed, i.e., transferring the knowledge in the 2D images to 3D point clouds. Specifically, we propose a novel approach for the challenging cross-modal and cross-domain adaptation task by fully exploring the relationship between images and point clouds and designing effective feature alignment strategies. Without any 3D labels, our method achieves state-of-the-art performance for 3D point cloud semantic segmentation on SemanticKITTI by using the knowledge of KITTI360 and GTA5, compared to existing unsupervised and weakly-supervised baselines.","tags":["LiDAR Segmentation","Domain Adaptation","Label-efficient"],"title":"Cross-Modal and Cross-Domain Knowledge Transfer for Label-Free 3D Segmentation","type":"publication"},{"authors":["__**Dai-Jie Wu**__","Pin-Yen Chiu","Chih-Yu Wang","Jun-Cheng Chen"],"categories":null,"content":"","date":1690848e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703399766,"objectID":"a4f07aa71fe04c1a1301effacd05eb60","permalink":"https://jaywukaust.github.io/publication/fg24eval/","publishdate":"2023-08-01T00:00:00Z","relpermalink":"/publication/fg24eval/","section":"publication","summary":"Face editing has recently blossomed into a highly active and significant domain, impacting numerous applications from entertainment to security. Despite its rapid growth, the field still faces challenges in establishing a universally accepted and robust evaluation mechanism that can comprehensively assess the performances of face editing techniques and their underlying generative models. To address this challenge, our paper introduces a well-defined evaluation protocol that seamlessly combines systematic experimental methodologies with thorough subjective evaluations. This collaborative approach ensures a more in-depth and unbiased examination of face editing techniques. Based on our extensive studies, we observe that traditional metrics, notably the Fréchet Inception Distance (FID), serve well in measuring perceptual attributes of edited images. However, they might fall short in covering all aspects of a face editing model's capabilities. To bridge this gap, we have incorporated additional metrics that assess disentanglement and editing effectiveness, leading to the creation of a holistic assessment framework that promises a more comprehensive evaluation upon face editing capability of different deep generative models.","tags":["Face Editing","Diffusion","StyleGAN"],"title":"Towards Validating Face Editing Ability in Generative Models","type":"publication"},{"authors":null,"categories":null,"content":"Magic123: One Image to High-Quality 3D Object Generation Using Diffusion Priors The awesome website template is borrowed from DreamFusion: Text-to-3D using 2D Diffusion and Nerfies website .\nWebsite License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703254028,"objectID":"0d42195cc2dae42fbc7bb9033a1a34bb","permalink":"https://jaywukaust.github.io/project/magic123/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/magic123/readme/","section":"project","summary":"Magic123: One Image to High-Quality 3D Object Generation Using Diffusion Priors The awesome website template is borrowed from DreamFusion: Text-to-3D using 2D Diffusion and Nerfies website .\nWebsite License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"","type":"project"}]